{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "# Initial system prompt\n",
    "system_prompt = \"\"\"You are provided with a code snippet and a question about this code. \n",
    "You are a tutor who explains the code and answers the question as prompted.\n",
    "Respond clearly and concisely, using examples to enhance understanding.\n",
    "\"\"\"\n",
    "\n",
    "# Questions and answers as objects in an array\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\",\n",
    "        \"answer\": \"\"\"\n",
    "This code snippet is a generator expression that yields unique authors from a list of dictionaries called `books`. \n",
    "1. `{book.get(\"author\") for book in books if book.get(\"author\")}`: Creates a set of authors by iterating over `books` and extracting the value associated with the \"author\" key, only if it exists.\n",
    "2. `yield from`: Allows the generator to yield elements from the set one by one. This avoids duplicates due to the set's nature.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"\n",
    "What does this code do, and why is it written this way?:\n",
    "squared = {x: x**2 for x in range(5)}\n",
    "\"\"\",\n",
    "        \"answer\": \"\"\"\n",
    "This code creates a dictionary using a dictionary comprehension. The dictionary maps each number in the range `[0, 4]` to its square.\n",
    "1. `range(5)`: Generates numbers from 0 to 4.\n",
    "2. `{x: x**2}`: For each `x`, the dictionary key is `x` and the value is `x**2`.\n",
    "The purpose is to efficiently compute and store the squares of numbers in a single step.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"\n",
    "What does this code do in the context of NLP?\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\"\"\",\n",
    "        \"answer\": \"\"\"\n",
    "This code uses the `transformers` library to tokenize a text input for NLP tasks:\n",
    "1. `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: Loads a pre-trained BERT tokenizer.\n",
    "2. `tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")`: Converts the input text into token IDs and returns the output as PyTorch tensors (`\"pt\"`).\n",
    "The output is ready for further processing in a deep learning model.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"\n",
    "How does this bubble sort implementation work, and what are its limitations?\n",
    "def bubble_sort(arr):\n",
    "    n = len(arr)\n",
    "    for i in range(n):\n",
    "        for j in range(0, n-i-1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "\"\"\",\n",
    "        \"answer\": \"\"\"\n",
    "This code implements the bubble sort algorithm:\n",
    "1. It iterates through the array multiple times (`for i in range(n)`) and compares adjacent elements.\n",
    "2. If an element is greater than the next, they are swapped.\n",
    "3. The largest elements \"bubble\" to the end of the array with each iteration.\n",
    "\n",
    "**Limitations**:\n",
    "- Time complexity: O(n^2) in the worst case, making it inefficient for large datasets.\n",
    "- It is not stable for sorting complex data structures unless explicitly handled.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add examples to the system prompt\n",
    "for i, example in enumerate(examples, start=1):\n",
    "    system_prompt += f\"\\n### Example {i}:\\n\"\n",
    "    system_prompt += f\"Question:\\n{example['question']}\\n\"\n",
    "    system_prompt += f\"Answer:\\n{example['answer']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d4fa47-4187-4d44-9cf2-3a0631e30e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a code snippet and a question about this code. \n",
      "You are a tutor who explains the code and answers the question as prompted.\n",
      "Respond clearly and concisely, using examples to enhance understanding.\n",
      "\n",
      "### Example 1:\n",
      "Question:\n",
      "\n",
      "Please explain what this code does and why:\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "\n",
      "Answer:\n",
      "\n",
      "This code snippet is a generator expression that yields unique authors from a list of dictionaries called `books`. \n",
      "1. `{book.get(\"author\") for book in books if book.get(\"author\")}`: Creates a set of authors by iterating over `books` and extracting the value associated with the \"author\" key, only if it exists.\n",
      "2. `yield from`: Allows the generator to yield elements from the set one by one. This avoids duplicates due to the set's nature.\n",
      "\n",
      "\n",
      "### Example 2:\n",
      "Question:\n",
      "\n",
      "What does this code do, and why is it written this way?:\n",
      "squared = {x: x**2 for x in range(5)}\n",
      "\n",
      "Answer:\n",
      "\n",
      "This code creates a dictionary using a dictionary comprehension. The dictionary maps each number in the range `[0, 4]` to its square.\n",
      "1. `range(5)`: Generates numbers from 0 to 4.\n",
      "2. `{x: x**2}`: For each `x`, the dictionary key is `x` and the value is `x**2`.\n",
      "The purpose is to efficiently compute and store the squares of numbers in a single step.\n",
      "\n",
      "\n",
      "### Example 3:\n",
      "Question:\n",
      "\n",
      "What does this code do in the context of NLP?\n",
      "from transformers import AutoTokenizer\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "tokens = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
      "\n",
      "Answer:\n",
      "\n",
      "This code uses the `transformers` library to tokenize a text input for NLP tasks:\n",
      "1. `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: Loads a pre-trained BERT tokenizer.\n",
      "2. `tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")`: Converts the input text into token IDs and returns the output as PyTorch tensors (`\"pt\"`).\n",
      "The output is ready for further processing in a deep learning model.\n",
      "\n",
      "\n",
      "### Example 4:\n",
      "Question:\n",
      "\n",
      "How does this bubble sort implementation work, and what are its limitations?\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(n):\n",
      "        for j in range(0, n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "\n",
      "Answer:\n",
      "\n",
      "This code implements the bubble sort algorithm:\n",
      "1. It iterates through the array multiple times (`for i in range(n)`) and compares adjacent elements.\n",
      "2. If an element is greater than the next, they are swapped.\n",
      "3. The largest elements \"bubble\" to the end of the array with each iteration.\n",
      "\n",
      "**Limitations**:\n",
      "- Time complexity: O(n^2) in the worst case, making it inefficient for large datasets.\n",
      "- It is not stable for sorting complex data structures unless explicitly handled.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the resulting system prompt\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de2e284-fbc6-459e-bcf0-c2b80d0067d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Can you tell me what this code snippet do and why?\n",
    "\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \n",
    "        # Create this Website object from the given url using the BeautifulSoup library\n",
    "        \n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b3328f-7134-45fe-8bae-e7744124be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages_for():\n",
    "    return [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "def get_answer_with_stream():\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages= get_messages_for(),\n",
    "        stream = True\n",
    "    )    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a0352e1-3dd9-4580-81ed-726a889840f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet defines a class `Website` that retrieves and processes data from a specified URL using the BeautifulSoup library. Here’s a breakdown of what it does and why it’s written this way:\n",
       "\n",
       "1. **Headers Dictionary**:\n",
       "   python\n",
       "   headers = {\n",
       "       \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
       "   }\n",
       "   \n",
       "   - This dictionary simulates a web browser request by specifying a `User-Agent`. This helps prevent the request from being blocked by some websites that restrict web scrapers.\n",
       "\n",
       "2. **Website Class Initialization**:\n",
       "   python\n",
       "   class Website:\n",
       "\n",
       "       def __init__(self, url):\n",
       "   \n",
       "   - The `Website` class is initialized with a URL. When an instance of this class is created, it fetches the web page content of the given URL.\n",
       "\n",
       "3. **Fetching URL Content**:\n",
       "   python\n",
       "   response = requests.get(url, headers=headers)\n",
       "   \n",
       "   - This line performs a GET request to the URL using the `requests` library, sending the custom headers defined earlier. It retrieves the page's content.\n",
       "\n",
       "4. **Parsing HTML with BeautifulSoup**:\n",
       "   python\n",
       "   soup = BeautifulSoup(response.content, 'html.parser')\n",
       "   \n",
       "   - The HTML content of the page is parsed using BeautifulSoup, allowing for easy navigation and manipulation of the HTML structure.\n",
       "\n",
       "5. **Extracting the Title**:\n",
       "   python\n",
       "   self.title = soup.title.string if soup.title else \"No title found\"\n",
       "   \n",
       "   - It attempts to retrieve the text of the page title by accessing `soup.title.string`. If the title is not present, it defaults to \"No title found\".\n",
       "\n",
       "6. **Cleaning Up Irrelevant Tags**:\n",
       "   python\n",
       "   for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
       "       irrelevant.decompose()\n",
       "   \n",
       "   - This loop removes irrelevant tags (such as scripts, styles, images, and input elements) from the body of the document. `decompose()` effectively removes these elements from the parsed structure.\n",
       "\n",
       "7. **Extracting Text Content**:\n",
       "   python\n",
       "   self.text = soup.body.get_text(separator=\"\n",
       "\", strip=True)\n",
       "   \n",
       "   - Finally, it extracts the clean text content from the body, using a newline as a separator and stripping extra whitespace. The result is stored in the `self.text` attribute.\n",
       "\n",
       "**Purpose**:\n",
       "The main purpose of this code is to create a `Website` object that encapsulates the URL, its title, and the text content, while stripping out unnecessary HTML elements. This makes it suitable for web scraping tasks, text analysis, or any application that requires clean text from web pages."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_answer_with_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "def get_answer_with_llama():\n",
    "    response = ollama.chat(model=MODEL_LLAMA, messages=get_messages_for())\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1e405ba-a587-4b32-8ca0-ee9fa4fabda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "\n",
       "This code snippet is used to scrape information from a given website. Here's what it does:\n",
       "\n",
       "1. **User Agent Header**: The `headers` dictionary sets the User-Agent header of the HTTP request made to the website, which simulates a browser making a request.\n",
       "\n",
       "2. **Website Class**: A class named `Website` is defined with an initializer (`__init__`) method that takes a URL as input.\n",
       "\n",
       "3. **Website Initialization**:\n",
       "    - The URL is stored in the object's `url` attribute.\n",
       "    - An HTTP GET request is sent to the website using the `requests.get()` function, passing the URL and the `headers` dictionary to set the User-Agent header.\n",
       "    - The server response is parsed using the `BeautifulSoup` library, which creates a parse tree from the HTML content of the webpage.\n",
       "\n",
       "4. **Parsing the Webpage**:\n",
       "    - The title of the webpage is extracted and stored in the object's `title` attribute. If no title exists, it defaults to \"No title found\".\n",
       "    - Irrelevant elements like scripts, styles, images, inputs, etc., are removed from the HTML content by calling their `decompose()` method.\n",
       "    - The text content of the webpage is extracted and stored in the object's `text` attribute. The `get_text()` method is used with an optional separator (`\" \"`) to split the text into lines, and `strip=True` to remove leading/trailing whitespace.\n",
       "\n",
       "**Purpose**\n",
       "\n",
       "The purpose of this code snippet is to create a simple web scraper that can extract basic information from a webpage, such as its title and text content. The User-Agent header is set to simulate a browser making a request, which may help avoid certain types of anti-scraping measures some websites employ.\n",
       "\n",
       "This code can be used in various applications like web data mining, social media monitoring, or even simple web testing tools. However, it's essential to note that web scraping should always be done responsibly and with the permission of the website owner, as many sites prohibit scraping in their terms of service."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(get_answer_with_llama()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063cc9f-57c4-4a38-95e8-14cab2a0a00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
